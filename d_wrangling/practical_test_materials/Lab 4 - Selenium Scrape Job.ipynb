{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d69432",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This practice covers the steps on how to scrape jobs from Foundit. Using python, selenium, and pandas, we'll be able to extract information from foundit.sg and construct a pandas data frame. Before we begin, let's understand web scraping simply. \n",
    "\n",
    "Imagine if you are trying to get much information about something from various web pages and articles that need to be stored in a suitable format, for instance, an excel file. One way is to go through all those websites and write the useful information to the excel sheets manually. But programmers tend to do it in an easy way which is web scraping. Web scraping is the technique of extracting a large amount of data from different web pages that can be stored in a suitable format.\n",
    "\n",
    "## Scraping job details from Foundit\n",
    "The foundit (formerly Monster) Job Search App is a platform for freshers & experienced job seekers to find their perfect career opportunities. \n",
    "\n",
    "Here are the steps involved:\n",
    "\n",
    "1. Install and import necessary modules\n",
    "2. Send some basic queries like like job title or company name and location to the Foundit website using selenium\n",
    "3. Fetch the current URL after sending the queries to the website using selenium\n",
    "4. Fetch the information about job title, company name, rating, location, simple description, date of posting, etc\n",
    "5. Store this information into a CSV file using pandas\n",
    "\n",
    "\n",
    "## Load Libraries\n",
    "First of all, we need to install some specific modules including a chrome driver for selenium. After installing the chrome driver move it to the working directory.\n",
    "\n",
    "We need to import the libraries that will be used for this practical. Here requests help to send an HTTP request using python, Selenium is an automation tool that helps here to send queries to the website, lxml can convert the page into XML or HTML format. Pandas is to convert the data into a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14059422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium requests webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fb12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puolsky/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b75fee",
   "metadata": {},
   "source": [
    "## Sending job title and location using selenium\n",
    "Now let's create a function that sends queries to the web page and returns the current URL. This function opens Foundit using the specified URL as one of its parameters. Then it sends the job title and location to the site using selenium. After that, we'll get a new page and its URL which consists of all the job details related to the job title and location you have specified as its parameters. Lastly, it returns the current URL which consists of jobs and their details so that we can simply scrape it using Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0922d3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.foundit.sg/srp/results?query=Data+Scientist&locations=Singapore&experienceRanges=3%7E3&experience=3\n"
     ]
    }
   ],
   "source": [
    "# Sending job title and location using selenium\n",
    "def get_current_url(url, job_title, location):\n",
    "\n",
    "    # adjust your code if you are using Microsoft Edge web browser\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.maximize_window()\n",
    "\n",
    "    time.sleep(3)\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    driver.find_element(\"xpath\", '//*[@id=\"heroSectionDesktop-skillsAutoComplete--input\"]').send_keys(job_title)    \n",
    "    \n",
    "    time.sleep(3)\n",
    "    driver.find_element(\"xpath\", '//*[@id=\"heroSectionDesktop-locationAutoComplete--input\"]').send_keys(location)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    driver.find_element(\"xpath\", '//*[@id=\"heroSectionDesktop-expAutoComplete--input\"]').click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    driver.find_element(\"xpath\", '//*[@id=\"searchDropDown\"]/ul/li[4]/span').click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    driver.find_element(\"xpath\", '//*[@id=\"searchForm\"]/div/button').click()\n",
    "    \n",
    "    current_url = driver.current_url\n",
    "    driver.quit()\n",
    "    return current_url\n",
    "\n",
    "current_url = get_current_url('https://foundit.sg/',\"Data Scientist\", \"Singapore\")\n",
    "print(current_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813607e2",
   "metadata": {},
   "source": [
    "## Scraping jobs using Beautiful Soup\n",
    "\n",
    "Not all the websites can use BeatuifulSoup to scrape. check https://www.foundit.sg/robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62e9445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "# Scraping jobs details\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/50.0.2661.102 Safari/537.36'\n",
    "}\n",
    "resp = requests.get(current_url)\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf50418",
   "metadata": {},
   "source": [
    "## Scraping jobs using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff0e7b",
   "metadata": {},
   "source": [
    "The next step is to find the CSS selectors and retrieve the raw text inside the tags that contain these CSS selectors. The CSS selectors given in the code are probably the same on the web page but sometimes it may change.\n",
    "\n",
    "By looping through all the job posts we'll get much information about it. Lastly, we converted the data into a pandas data frame and simply returned it. You'll get the details about the job title, company name, salary, post date and experience. You can save it as a CSV file using df.to_csv(\"jobs.csv\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9346983e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_job_details(url):\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    content = driver.find_elements(By.CLASS_NAME, 'srpResultCardContainer')\n",
    "    jobs_list = []\n",
    "    for post in content:\n",
    "        try:\n",
    "            data = {\n",
    "                \"job_title\": post.find_element(By.CLASS_NAME, 'jobTitle').text,\n",
    "                \"company\": post.find_element(By.CLASS_NAME, 'companyName').text,\n",
    "                \"date\": post.find_element(By.CLASS_NAME, 'timeText').text,\n",
    "                \"experience\": post.find_elements(By.CLASS_NAME, 'details')[0].text\n",
    "            }\n",
    "        except IndexError:\n",
    "            continue\n",
    "        jobs_list.append(data)\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs_list)\n",
    "\n",
    "df_jobs = scrape_job_details(current_url)\n",
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708e23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.to_csv('jobs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
