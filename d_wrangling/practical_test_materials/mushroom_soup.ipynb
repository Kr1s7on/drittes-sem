{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9072a711",
   "metadata": {},
   "source": [
    "```css\n",
    "\n",
    " ___ ___  _ _  ___  ___\n",
    "<_-</   \\| | ||   \\<_-<\n",
    "/__/\\___/`___||  _//__/\n",
    "              |_|      \n",
    "\n",
    "```\n",
    "~ material for beautifulsoup ‧₊˚ ⋅ 𓐐𓎩 ‧₊˚ ⋅\n",
    "\n",
    "₊˚ ⋅ table of contents ₊˚ ⋅ \n",
    "* [basics](#the-basics)\n",
    "* [finding elements](#section-1)\n",
    "* [using css selectors](#using-css-selectors)\n",
    "* [extracting data from elements](#extracting-data-from-elements)\n",
    "* [scraping example](#scraping-example)\n",
    "* [exercise one](#exercise-one)\n",
    "* [exercise two](#exercise-two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 lxml pandas openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8fbe7",
   "metadata": {},
   "source": [
    "## the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f7bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Example Domain\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <style type=\"text/css\">\n",
      "   body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "  </style>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <h1>\n",
      "    Example Domain\n",
      "   </h1>\n",
      "   <p>\n",
      "    This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.\n",
      "   </p>\n",
      "   <p>\n",
      "    <a href=\"https://www.iana.org/domains/example\">\n",
      "     More information...\n",
      "    </a>\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Step 1: Get the HTML content ---\n",
    "URL = \"https://example.com/\"\n",
    "\n",
    "# It's good practice to include headers to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'DNT': '1',  # Do Not Track Request Header\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(URL, headers=headers, timeout=10) # timeout in seconds\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4XX or 5XX)\n",
    "    html_content = response.content # Use .content for binary response (bytes), .text for text\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching URL {URL}: {e}\")\n",
    "    # exit() # Or handle error appropriately\n",
    "\n",
    "# --- Step 2: Parse the HTML ---\n",
    "# Common parsers: 'lxml' (fast, needs pip install lxml), 'html.parser' (Python built-in, slower)\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "# soup = BeautifulSoup(html_content, 'html.parser') # Alternative\n",
    "\n",
    "# --- Step 3: (Optional) Prettify to inspect structure (for debugging) ---\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5933b1",
   "metadata": {},
   "source": [
    "## finding elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59e234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First <p>: This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.\n",
      "Page Title: title - Example Domain\n"
     ]
    }
   ],
   "source": [
    "# Find the first occurrence of a tag\n",
    "first_p_tag = soup.find('p')\n",
    "if first_p_tag:\n",
    "    print(f\"First <p>: {first_p_tag.text}\")\n",
    "\n",
    "# Find all occurrences of a tag (returns a list-like ResultSet)\n",
    "all_a_tags = soup.find_all('a')\n",
    "\n",
    "# Print all <a> tags with their text and href attributes\n",
    "# for a_tag in all_a_tags:\n",
    "#     print(f\"Link text: {a_tag.text}, Href: {a_tag.get('href')}\")\n",
    "\n",
    "# Get the page title\n",
    "page_title = soup.title\n",
    "if page_title:\n",
    "    print(f\"Page Title: {page_title.name} - {page_title.string}\") # .name is 'title', .string is the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca3d57",
   "metadata": {},
   "source": [
    "## using css selectors\n",
    "`select()` and `select_one()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c12284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select by Tag ---\n",
    "all_paragraphs_css = soup.select('p')\n",
    "\n",
    "# --- Select by Class ---\n",
    "elements_with_class_css = soup.select('.someClassName') # Note the leading dot for class\n",
    "\n",
    "# --- Select by ID ---\n",
    "element_with_id_css = soup.select_one('#uniqueElementId') # Note the leading hash for ID\n",
    "\n",
    "# --- Select by Attribute ---\n",
    "# Tag with an attribute\n",
    "links_with_href = soup.select('a[href]')\n",
    "\n",
    "# Tag with a specific attribute value\n",
    "specific_link = soup.select_one('a[href=\"http://example.com\"]')\n",
    "\n",
    "# Tag with an attribute starting with a value\n",
    "links_starting_with_https = soup.select('a[href^=\"https://\"]')\n",
    "\n",
    "# Tag with an attribute ending with a value\n",
    "image_files = soup.select('img[src$=\".jpg\"]')\n",
    "\n",
    "# Tag with an attribute containing a value\n",
    "links_containing_search = soup.select('a[href*=\"search\"]')\n",
    "\n",
    "# --- Descendant Combinator (space) ---\n",
    "# Selects <a> tags that are descendants of a <div> with class 'container'\n",
    "links_in_container = soup.select('div.container a')\n",
    "\n",
    "# --- Child Combinator (>) ---\n",
    "# Selects <p> tags that are direct children of a <div> with ID 'content'\n",
    "paragraphs_in_content_div = soup.select('div#content > p')\n",
    "\n",
    "# --- Adjacent Sibling Combinator (+) ---\n",
    "# Selects the <p> immediately following an <h2>\n",
    "p_after_h2 = soup.select_one('h2 + p')\n",
    "\n",
    "# --- General Sibling Combinator (~) ---\n",
    "# Selects all <p> siblings that follow an <h2>\n",
    "all_p_siblings_after_h2 = soup.select('h2 ~ p')\n",
    "\n",
    "# --- Multiple Selectors (comma separated) ---\n",
    "# Selects all <h2> tags AND all <p> tags with class 'important'\n",
    "headings_and_important_text = soup.select('h2, p.important')\n",
    "\n",
    "# --- Example usage with select_one ---\n",
    "first_article_title = soup.select_one('article.post h1.title')\n",
    "# if first_article_title:\n",
    "# print(f\"First article title: {first_article_title.text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31bb57",
   "metadata": {},
   "source": [
    "## extracting data from elements\n",
    "Once you have a specific HTML tag (e.g., from `soup.find(...)` or `soup.select_one(...)`), here's how to get information out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'element' is a Tag object returned by find() or select_one()\n",
    "# Example: element = soup.find('a', class_='my-link')\n",
    "\n",
    "# --- Get Text Content ---\n",
    "# .text: Returns all text within the tag, including children, stripped of whitespace at ends.\n",
    "# .string: If a tag has only one child and it's a NavigableString, .string gives that string.\n",
    "#          Otherwise, it's None. Safer to use .text or .get_text().\n",
    "# .get_text(): More robust. Can specify a separator for text from different tags.\n",
    "#              .get_text(strip=True) removes whitespace from start/end of each string segment\n",
    "#              before joining.\n",
    "\n",
    "if element:\n",
    "    print(f\"Text: {element.text.strip()}\") # .strip() is common to remove leading/trailing whitespace\n",
    "    print(f\"Text (get_text, strip=True): {element.get_text(strip=True)}\")\n",
    "    print(f\"Text (get_text with separator): {element.get_text(separator=' | ', strip=True)}\")\n",
    "\n",
    "# --- Get Attribute Values ---\n",
    "# Use dictionary-like access or .get() method\n",
    "if element and element.name == 'a': # Check if it's an 'a' tag for 'href'\n",
    "    link_url = element['href'] # Raises KeyError if 'href' doesn't exist\n",
    "    link_url_safe = element.get('href') # Returns None if 'href' doesn't exist\n",
    "    data_id = element.get('data-id')\n",
    "\n",
    "    print(f\"Href (direct): {link_url}\")\n",
    "    print(f\"Href (safe .get()): {link_url_safe}\")\n",
    "    print(f\"Data-ID: {data_id}\")\n",
    "\n",
    "# --- Get Tag Name ---\n",
    "if element:\n",
    "    print(f\"Tag Name: {element.name}\")\n",
    "\n",
    "# --- Get All Attributes as a Dictionary ---\n",
    "if element:\n",
    "    print(f\"All attributes: {element.attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982350a",
   "metadata": {},
   "source": [
    "## scraping example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345aa9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: The Dormouse's story\n",
      "Sisters: [{'name': 'Elsie', 'link': 'http://example.com/elsie'}, {'name': 'Lacie', 'link': 'http://example.com/lacie'}, {'name': 'Tillie', 'link': 'http://example.com/tillie'}]\n",
      "Articles: [{'title': 'Article 1', 'content_preview': 'Content for article 1.Read more...'}, {'title': 'Article 2', 'content_preview': 'Content for article 2.Read more...'}]\n"
     ]
    }
   ],
   "source": [
    "# Sample HTML (replace with actual fetching if needed)\n",
    "sample_html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<div class=\"article\">\n",
    "  <h2>Article 1</h2>\n",
    "  <p>Content for article 1. <a href=\"/article1-details\">Read more...</a></p>\n",
    "</div>\n",
    "<div class=\"article\">\n",
    "  <h2>Article 2</h2>\n",
    "  <p>Content for article 2. <a href=\"/article2-details\">Read more...</a></p>\n",
    "</div>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup_sample = BeautifulSoup(sample_html_doc, 'lxml')\n",
    "\n",
    "# With actual url\n",
    "# soup_sample = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "# --- Extract title ---\n",
    "title = soup_sample.title.string\n",
    "print(f\"Page Title: {title}\")\n",
    "\n",
    "# --- Extract all sister names and links ---\n",
    "sisters_data = []\n",
    "sister_tags = soup_sample.select('a.sister') # Using CSS selector\n",
    "for sister_tag in sister_tags:\n",
    "    name = sister_tag.get_text(strip=True)\n",
    "    link = sister_tag.get('href')\n",
    "    sisters_data.append({'name': name, 'link': link})\n",
    "print(f\"Sisters: {sisters_data}\")\n",
    "\n",
    "# --- Extract article titles and their first paragraph ---\n",
    "articles_data = []\n",
    "article_divs = soup_sample.select('div.article')\n",
    "for article_div in article_divs:\n",
    "    article_title_tag = article_div.select_one('h2') # Find h2 within this article_div\n",
    "    article_content_tag = article_div.select_one('p') # Find p within this article_div\n",
    "    \n",
    "    article_title = article_title_tag.get_text(strip=True) if article_title_tag else \"N/A\"\n",
    "    article_content = article_content_tag.get_text(strip=True) if article_content_tag else \"N/A\"\n",
    "    \n",
    "    articles_data.append({'title': article_title, 'content_preview': article_content})\n",
    "print(f\"Articles: {articles_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4b184",
   "metadata": {},
   "source": [
    "### if pagination is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1, 4):\n",
    "    url = f\"https://example.com/news?page={page}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    # then same as above..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2330b79",
   "metadata": {},
   "source": [
    "## exercise one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175cfc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Title                 Author\n",
      "0        A CURSE CARVED IN BONE  by Danielle L. Jensen\n",
      "1                    THE TENANT     by Freida McFadden\n",
      "2       THE EMPEROR OF GLADNESS         by Ocean Vuong\n",
      "3      GREAT BIG BEAUTIFUL LIFE         by Emily Henry\n",
      "4              CAN'T GET ENOUGH        by Kennedy Ryan\n",
      "5             ONE GOLDEN SUMMER      by Carley Fortune\n",
      "6                    THE DEVILS     by Joe Abercrombie\n",
      "7                   FEVER BEACH        by Carl Hiaasen\n",
      "8   REMARKABLY BRIGHT CREATURES     by Shelby Van Pelt\n",
      "9            SHIELD OF SPARROWS        by Devney Perry\n",
      "10                   MY FRIENDS     by Fredrik Backman\n",
      "11                   ONYX STORM      by Rebecca Yarros\n",
      "12          MARBLE HALL MURDERS    by Anthony Horowitz\n",
      "13                  FOURTH WING      by Rebecca Yarros\n",
      "14          I HOPE YOU REMEMBER         by Josie Balka\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# webpage url\n",
    "url = \"https://www.nytimes.com/books/best-sellers/combined-print-and-e-book-fiction/\"\n",
    "\n",
    "# send req to the url\n",
    "response = requests.get(url)\n",
    "\n",
    "# converting response to BS object\n",
    "content = BeautifulSoup(response.content, 'html')\n",
    "\n",
    "# find the table with class 'css-12yzwg4'\n",
    "books = content.find_all('li', {\"class\": \"css-sggj6j\"})\n",
    "\n",
    "# loop through each book and extract details\n",
    "book_list = []\n",
    "for book in books:\n",
    "\n",
    "    # book title\n",
    "    title = book.find('h3', {\"class\": \"css-2jegzb\"}).get_text(strip = True)\n",
    "\n",
    "    # book author\n",
    "    author = book.find('p', {\"class\": \"css-1aaqvca\"}).get_text(strip = True)\n",
    "\n",
    "    data = {\n",
    "        'Title': title,\n",
    "        'Author': author,\n",
    "    }\n",
    "\n",
    "    book_list.append(data)\n",
    "\n",
    "book_list_df = pd.DataFrame(book_list)\n",
    "print(book_list_df)\n",
    "\n",
    "# Save to CSV\n",
    "# book_list_df.to_csv('best_selling_books.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ba1c6",
   "metadata": {},
   "source": [
    "## exercise two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f02738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Name Original Price  \\\n",
      "0                INDEX AGENT OFFICE CHAIR (HIGH BACK)       S$359.00   \n",
      "1   COOLERMASTER CMI-GCR2C-GY CALIBER R2C GAMING C...       S$499.00   \n",
      "2   COOLERMASTER CMI-GCX1C-GY CALIBER X1C GAMING C...       S$599.00   \n",
      "3           ORTHO BACK BACK SUPPORT - ASSORTED COLOUR                  \n",
      "4                             JOURNALIST OFFICE CHAIR       S$179.00   \n",
      "5                       INDEX TEMPO MESH OFFICE CHAIR       S$189.00   \n",
      "6                            MORGEN MESH CHAIR (BLUE)       S$199.00   \n",
      "7                           MORGEN MESH CHAIR (BLACK)       S$199.00   \n",
      "8                    HEALING ORTHO BACK FOLDING CHAIR                  \n",
      "9        CURVO HOME OFFICE CHAIR BLACK MID BACK CHAIR                  \n",
      "10        CURVO HOME OFFICE CHAIR GREY MID BACK CHAIR                  \n",
      "11             INDEX EXECUTIVE HIGH BACK OFFICE CHAIR       S$289.00   \n",
      "12                 MOLLER HI BACK DIRECTOR MESH CHAIR                  \n",
      "13                 CELLO MID BACK STUDY CHAIR - BLACK       S$676.00   \n",
      "14                            INDEX GRIS OFFICE CHAIR       S$379.00   \n",
      "15               INDEX FORUMER HIGH BACK OFFICE CHAIR       S$349.00   \n",
      "\n",
      "   Current Price  \n",
      "0       S$199.00  \n",
      "1       S$349.00  \n",
      "2       S$399.00  \n",
      "3        S$77.00  \n",
      "4        S$89.00  \n",
      "5       S$129.00  \n",
      "6       S$139.00  \n",
      "7       S$139.00  \n",
      "8       S$148.00  \n",
      "9       S$198.00  \n",
      "10      S$198.00  \n",
      "11      S$199.00  \n",
      "12      S$228.00  \n",
      "13      S$228.00  \n",
      "14      S$229.00  \n",
      "15      S$229.00  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.courts.com.sg/furniture/furniture/study-desks\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Converting response to BS object\n",
    "content = BeautifulSoup(response.content, 'html') # or 'lxml'\n",
    "\n",
    "products = content.find_all('li', {\"class\": \"item product product-item\"})\n",
    "\n",
    "product_list = []\n",
    "for product in products:\n",
    "\n",
    "    # Product name\n",
    "    product_name = product.find('h3', {\"class\": \"product name product-item-name\"}).get_text(strip=True)\n",
    "\n",
    "    # Current product price\n",
    "    current_price_content = product.find('span', {\"class\": \"special-price\"})\n",
    "\n",
    "    #  Check for special price, if not found, use regular price\n",
    "    if current_price_content is not None:\n",
    "        current_price = current_price_content.find('span', {\"class\": \"price\"}).get_text(strip=True)\n",
    "    else:\n",
    "        current_price = product.find('span', {\"class\": \"price\"}).get_text(strip=True)\n",
    "\n",
    "    # Original product price\n",
    "    # og_price_content = product.find('span', {\"class\": \"old-price\"})\n",
    "    # if og_price_content is not None:\n",
    "    #     og_price = \"\"\n",
    "    # else:\n",
    "    #     og_price = og_price_content.find('span', {\"class\": \"price\"}).get_text(strip=True)\n",
    "\n",
    "    og_price_content = product.find('span', {\"class\": \"old-price\"})\n",
    "    if og_price_content is not None:\n",
    "        og_price = og_price_content.find('span', {\"class\": \"price\"}).get_text(strip=True)\n",
    "    else:\n",
    "        og_price = \"\"\n",
    "\n",
    "    data = {\n",
    "        'Name': product_name,\n",
    "        'Original Price': og_price,\n",
    "        'Current Price': current_price,\n",
    "    }\n",
    "\n",
    "    product_list.append(data)\n",
    "\n",
    "product_list_df = pd.DataFrame(product_list, index=False)\n",
    "print(product_list_df)\n",
    "\n",
    "# NOTE: Other saving options\n",
    "# Save to Excel\n",
    "# product_list_df.to_excel('study_desks.xlsx', index=False, engine='openpyxl')\n",
    "# Save to CSV\n",
    "# product_list_df.to_csv('study_desks.csv', index=False)\n",
    "# Save to JSON\n",
    "# product_list_df.to_json('study_desks.json', orient='records', lines=True)\n",
    "# Save to txt\n",
    "# product_list_df.to_csv('study_desks.txt', index=False, sep='\\t') # Tab-separated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
